import subprocess
import os
import shutil
import tempfile
import re
from orchestrator.graph.state import GraphState

def pyspark_node(state: GraphState) -> dict:
    """
    Bridge for the PySpark Agent.
    Creates a temporary environment, runs the standalone Python agent, 
    and returns extracted lineage for merging.
    """
    # 1. Setup paths to the standalone PySpark agent
    agent_dir = os.path.abspath(os.path.join(
        os.path.dirname(__file__), 
        "../../../agents/PySpark-agent/agent-analyzer"
    ))
    # Path to the specific output file generated by your agent
    output_file_path = os.path.join(agent_dir, "lineage-output.txt")
    
    # Clean up any old results before starting
    if os.path.exists(output_file_path): 
        os.remove(output_file_path)
    
    temp_repo_dir = tempfile.mkdtemp()
    
    try:
        # 2. Filter for PySpark files (.py)
        pyspark_files = [f for f in state.get("files", []) if f["filename"].endswith(".py")]
        
        if not pyspark_files:
            print("ℹ️ No PySpark files detected. Skipping node.")
            return {"lineage": []}

        # 3. Write files to temporary repository for the agent to scan
        for file_entry in pyspark_files:
            file_path = os.path.join(temp_repo_dir, file_entry["filename"])
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            with open(file_path, "w") as f: 
                f.write(file_entry["content"])

        # 4. Execute the Standalone Agent using run_analyzer.py
        # We use the full path to your standalone script
        command = f'python run_analyzer.py {temp_repo_dir}'
        
        print(f"--- [PySparkNode] Analyzing {len(pyspark_files)} Python file(s) ---")
        
        # We pass the existing environment to ensure .venv and API keys are available
        subprocess.run(command, cwd=agent_dir, shell=True, timeout=300)

        # 5. Parse the lineage-output.txt results
        pyspark_lineage = []
        if os.path.exists(output_file_path):
            with open(output_file_path, "r") as f:
                content = f.read()
            
            # Use regex to split sections by "File:" keyword, same as Hive/Scala
            sections = re.split(r"File:\s*", content)
            for section in sections[1:]:
                lines = section.strip().split('\n')
                filename = lines[0].strip()
                
                # Regex to find the READS and WRITES lines
                reads_match = re.search(r"READS:\s*(.*)", section)
                writes_match = re.search(r"WRITES:\s*(.*)", section)
                
                def clean_list(text): 
                    if not text or "(none)" in text.lower() or "error" in text.lower(): 
                        return []
                    # Split by comma and strip whitespace
                    return [item.strip() for item in text.split(",")]
                
                pyspark_lineage.append({
                    "file": filename,
                    "reads": clean_list(reads_match.group(1) if reads_match else ""),
                    "writes": clean_list(writes_match.group(1) if writes_match else "")
                })
            
            print(f"✅ Extracted lineage for {len(pyspark_lineage)} PySpark file(s).")

        # 6. Return only the new lineage data; LangGraph's reducer handles the list addition
        return {"lineage": pyspark_lineage}
            
    except Exception as e:
        print(f"❌ PySpark Bridge failed: {e}")
        return {"lineage": []}
    finally:
        # Always clean up the temporary directory
        shutil.rmtree(temp_repo_dir)